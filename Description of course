Libraries used-
numpy -> Mathematical work.
matplotlib.pyplot -> Plotting.
pandas -> Import and manage datasets.
sklearn.preprocessing -> Used to manage data (missing fields and categorize data).


Video short summary- 

### SECTION 2 ###

import libraries -> Import statements are written.
import datasets -> Dataset is read using pd.read_csv('filename') and we make X independent variable and y dependent variable tables using iloc.
missing data -> Imputer subclass is used to identify missing field and replacing it by mean of the remaining fields.
categorical data -> fit_transform method of LabelEncoder and OneHotEncoder subclass's is used to convert categorical variables(text) to numbers.(dummy variables)
splitting data -> from cross_validataion library train_test_split is used to make test set and the train set.
feature scaling -> StandardScaler is used to scale the data in range of 0 to 1 to improve accuracy.

### SECTION 4 ###

simple linear regression intuition -> y = m*x + c  or  y = b0 + b1x1
									  y - dependent variable
									  x - independent variable
									  b1 - coefficient of x
									  b0 - constant


simple linear regression using python - Test and train sets are made.
										Fitting slr to training set using LinearRegression.(regressor.fit is used)
										Predicting the test set results using [y_pred is the vector containing predictions of dependent 		variable(in this case salary of all test cases)].(regressor.predict is used)
										Visualising the training test data results using matplotlib.pyplot.

### SECTION 5 ###
Buisness problem ->
					50 startups info given:
					1.profits[dependent var]. 
					2.administration cost[independent var]. 
					3.R and D Spend[independent var].
					4.Marketing Spend[independent var]. 
					5.State.
						
					Goal is to find startup that is:
									1. Where companies perform better ?
									2. How much do the the good companies spend money ?
									3. Companies spend more on R and D or Marketing ?( Better results of profit )

Step 1 ->
			MLR equation -> y = b0 + b1*x1 + b2*x2 +...+ bn*xn

Step 2 ->	Assumptions of linear regression-
			1.Linearity.
			2.Homoscedaticity.
			3.Multivariate normality.
			4.Independance of errors.
			5.Lack of multicollinearity.

Step 3 ->	Assigning dummy varibles.
			y = b0 			+ 	b1*x1 		+ 		b2*x2     +  b3*x3     +   b4*D1 
				profits 		R&d Spend 		   admin        Marketing 		State
			
			Here we assigned variables to the data .
			
			The state variable is categorical so we made dummy variable(D1) so that we can use it in our equation.(1-new york , 0- california)

Step 4 ->	Dummy variable trap -   Always omit one dummy variable.
									Model gets confused between b0 , b4 and b5(if you don't omit) as all point towards the same thing.

P - Value -> It tells us the probability of getting that result if H0 is true. 
This refers to the Hypothesis test-
H0 - Null Hypothesis
H1 - Alternative Hypothesis.
if p value small -> it suggests(gives strong evidence) that results are significantly different from H0 , hence is wrong.
if p value large -> it suggests H0 is probably correct but isn't confirmed (unconfirmed / no evidence).
The p-value is NOT the probability the claim is true. 
The p-value is NOT the probability the null hypothesis is true.

Step 5 ->
			Methods of building a model-
			1.All-in.
			2.backward elimination.			|
			3.Forward selection.			|-> Aka Stepwise regression.
			4.Bidirectional Elimination.	|
			5.Score Comparism.


			All-in -> 	Add all variables in .
						Done if prior knowledge about variables being the predictors.

			Backward elimination -> 
				Steps:
				1. Select a significane level(SL) to stay in model.
				2. Fit the model with all predictors(all-in).
				3. Consider predictor with highets p-value(P) if P>SL go to step 4 or else finish.
				4. Remove that predictor.
				5. Fit the model without this predictor(We have to rebuilt the whole model again without that said variable).
				
				Continue steps 3 to 5 till P<SL is satisfied ie the highest p-value of the variables in the model is less than the significane level initially decided.

				Once satisfied the model is ready .(all var left have P<SL)

			Forward selection ->
				Steps:
				1. Select a significane level(SL) to stay in model.
				2. Fit a simple reg model for each variable  ie y ~ x1, y ~ x2, ... y ~ xn. Select the model with lowest p-value.
				3. Now contruct all 2 variable linear reg cases keeping 1 var constant ie the above selected var(from the selected model).
 				4. Consider the predictor with the lowest p-value . If P < SL go to step 3 or else Finish.

 				Continue steps 3 and 4 till the condition  P > SL satisfies and when it does "KEEP THE PREVIOUS MODEL" that consists of all p-values < SL .

 			Bidirectional Elimination-
 				Step:
 				1. Select a significane level to enter(SLENTER) and stay(SLSTAY) in model.[Genearlly 5% ie 0.05]
 				2. Do Forward selection:
 						Regress Y on each predictor var and predictor var with lowest p value is added to the model as long as P < SLENTER.
 				3. Do Backward elimination:
 						Since p values change as additional predictor variables are added, you have to check to see if any of the predictor variables in the model now have a p value greater than PEXIT , if so then remove the var that has highest p-value.
 				4. Repeat steps 2 and 3 till no new varibles can enter and no old variables can exit.
 				5. Model is ready.


In brief -
(1) Forward selection : It builds model by selecting the variables, one by one, which have lowest p-value less than significance level. (In other words, select a variable, which best improves the model).

(2) Backward elimination : It starts with all variables, and then removes one variable at a time, which has highest p-value greater than significance level. In other words, remove variable, that is least required in the model.

(3) Bidirectional elimination : Refer https://www.spcforexcel.com/knowledge/root-cause-analysis/stepwise-regression 


Reference above->
Goodness of fit:
The goodness of fit test is a statistical hypothesis test to see how well sample data fit a distribution from a population with a normal distribution. 
Put differently, this test shows if your sample data represents the data you would expect to find in the actual population or if it is somehow skewed.
Goodness-of-fit tests are statistical tests aiming to determine whether a set of observed values match those expected under the applicable model.


Multi Linear Regression in python:
		> Preprocessing template is used. 
			- Encoding of dummy variable is done.
			- Splitting of data
		> Fitting MultiLinear Reg is done.
		> Predicting the test set results.
		> Backward Elimination is done using statsmodel.formula.api (homework).
		> Code for Automatic Backward Elimination is discussed.



### SECTION 6 ###

Polynomial Linear regression inuition :
	
		y = b0 + b1*x1 + b2*x1^2 + b3*x1^3 ...


	Step 1:
			> Set working directory .
			> Add Preprocessing template.
			> Dataset name updated "position_salaries.csv". [position, level, salary]
			> Problem Statement - You are human resource team working for a big company and are about to hire a new employee in this company. Now the offer is mad eand negotaition starts about the salary of the future employee. he tells his salary of the last company and expects to be paid more than that. To verify this they contact the previous company and in response they get a list of positions and the salary. One of the HR guy says he can build a bluffing detector to verify the guys statement using polynomial regression. As the levels increase the salary increase . The levels are 1-10 and this model is used to find salary at level 6.5 .
			> Only 2 column are need :
				Level and Salary as only they are correlated and useful to find the result.
				Level is x and salary is y.

	Step 2:
			> Tables are made for x and y from the dataset.
			> As we have to make very accurate prediction and there are very low cases. We can't train the model in this case.
			> No additional feature scaling.
			> We are building linear reg and poly reg to compare both models.
			> For poly reg new class is used:
				from sklearn.preprocessing import PolynomialFeatures
			> An object is made that will be a transformer to tool that transforms a matrix of X to a matrix of exponentially increasing X values    [X_poly].
			> PolynomialFeatures(degree = 2) 2 stands for 2-dimensional.
			> In linear reg fit method is used but here we use fit_transform method to transform object to x and then transform X to X_poly
			> Now an lin model is created using (X_poly, y) which is our poly reg model.

	Step 3:
			> Visualizing the linear reg and poly reg model.
			> plt.scatter is used to plot the graph. 
			> A colour is added to distinguish between the dataset graph and model of the graph.(red - dataset, blue - model)
			> Linear Reg:
						plt.scatter(X, y, color = 'red')
						plt.plot(X, lin_reg.predict(X), color = 'blue')
						plt.title("Truth or Bluff linear reg") 
						plt.xlabel('Position level')
						plt.ylabel('Salary')
						plt.show()
			> Poly Reg:
						plt.scatter(X, y, color = 'red')
						plt.plot(X, lin_reg2.predict(X_poly), color = 'blue')
						plt.title("Truth or Bluff Poly reg") 
						plt.xlabel('Position level')
						plt.ylabel('Salary')
						plt.show()

			> Here, X_poly and lin_reg2 is taken .
			> Model is much better than lin reg but to improve it further we add a degree to the model .(degree = 3)
			> At degree = 4 we get a perfect graph plotting all points exactly. 
			> X_grid is added to improve accuracy.
			> Arrange and reshape are used here.

	Step 4:
			> Predicting the result.
			
			> Linear reg :
						lin_reg.predict(6.5)
			
			> Poly reg :
						lin_reg2.predict(poly_reg.fit_transform(6.5))
			
			> He said 160k salary and result is 158k salary so bluff is true.

	Regression template is a made .
			
			> Importing dataset, Splitting od dataset, Feature scaling, Fitting of the model, Predicting of new result and Visualizing of result     (plotting).

			> We created an additional part where we can opt for higher resolution and smoother curve using np.arrange and np.reshape.s


### SECTION 7 ###

SVR Inuition:
		
		> SVR supports both linear and non linear reg.
		> Main keywords used are :
				Kernel, Hyperplane, boundary line, epsilon(e), margin .

SVR python code: 
		
		> Pre processing template.(feature scaling must be done as it is not included in svr class)
		> from sklearn.svm import svr
		> Types of kernel:(kernel - function used to map lower dimension data into high dimension data.)
				1. default = rbf.
				2. poly.
				3. linear.
				4. sigmoid.
				5. precomputed or callable.
		> To find the excat salary transform method is used.
		> In the argument we have to add a array value so, 6.5 is converted to numpy array np.array(([6.5])).
		> Prediction of original scale is obtained by using inverse of the sv_y variable.
		

### SECTION 9 ###

Random Forest Regression:
	
	Intuition:
		
		> It is a version of ensemble learning(combining algo's or repetative addition of same algo's to form a much powerful model as compared to the original basic model).
		> Steps:
			1. Pick k data points from training set.
			2. Build the decision tree associated to thesek data points.
			3. Choose the number of trees you want to build and repeat steps 1 & 2.
			4. For a new data point, make each one of your N trees predict the value of Y for the data in question. And assign the new data point the average accross all of the predicted Y values.

	Code in python:

		> Set working directory.
		> Take regression template.
		> Import Class, create class object and fit model.
		> from sklearn.ensemble import RandomForestRegressor 
		> Parameters used n_estimator & random_state.
		> Non continous model received.
		> Tested on different no. of estimators(10, 100 & 300).


### SECTION 10 ###

Evaluating Regression Models Performance.

	R-Squared Intuition:

		> R-Squared, also known as the Coefficient of Determination, is a value between 0 and 1 that measures how well our regression line fits our data. R-Squared can be interpreted as the percent of variance in our dependent variable that can be explained by our model.
		> R^2 = 1 - (SSres / SStot)
		> R-Squared checks to see if our fitted regression line will predict y better than the mean will.
		> Aim to minimize SSresidual. Closer to 1 is better.
	
	Adjusted R-Squared Intuition:

		> Similar to R-squared, the Adjusted R-squared measures the variation in the dependent variable (or target), explained by only the features which are helpful in making predictions. Unlike R-squared, the Adjusted R-squared would penalize you for adding features which are not useful for predicting the target.
		> 